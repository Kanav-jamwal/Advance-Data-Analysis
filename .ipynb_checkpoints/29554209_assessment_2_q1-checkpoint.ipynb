{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Kanav Jamwal\n",
    "# Student ID: 29554209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A. Document Clustering\n",
    "## EM for Document Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(SnowballC)\n",
    "library(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of Hard EM and Soft EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Gholamreza Haffari and Borhan Kazimipour, July 2016\n",
    "#======\n",
    "#You are welcome to use this code for your submission if you would like to; otherwise, you are welcome to use your own code that you have developed so far. \n",
    "\n",
    "# The main aim is to help you folks to move forward to interesting parts of the Assessment, and to not get stuck in some implementation details.\n",
    "# Carefully note the comments throughout the code, particularly theo nes relating to handle small numbers in the log space and how to prevent NaN \n",
    "# when normalising a non-negative vector to get probability distribution. \n",
    "#========\n",
    "\n",
    "\n",
    "# You may need to install some packages:\n",
    "#install.packages('tm')\n",
    "#install.packages('SnowballC')\n",
    "\n",
    "eps=1e-10\n",
    "  \n",
    "# reading the data\n",
    "read.data <- function(file.name='./Task2A.txt', sample.size=1000, seed=100, pre.proc=TRUE, spr.ratio= 0.90) {\n",
    "  # INPUTS:\n",
    "  ## file.name: name of the input .txt file\n",
    "  ## sample.size: if == 0  reads all docs, otherwise only reads a subset of the corpus\n",
    "  ## seed: random seed for sampling (read above)\n",
    "  ## pre.proc: if TRUE performs the preprocessing (recommended)\n",
    "  ## spr.ratio: is used to reduce the sparcity of data by removing very infrequent words\n",
    "  # OUTPUTS:\n",
    "  ## docs: the unlabled corpus (each row is a document)\n",
    "  ## word.doc.mat: the count matrix (each rows and columns corresponds to words and documents, respectively)\n",
    "  ## label: the real cluster labels (will be used in visualization/validation and not for clustering)\n",
    "  \n",
    "  # Read the data\n",
    "  text <- readLines(file.name)\n",
    "  # select a subset of data if sample.size > 0\n",
    "  if (sample.size>0){\n",
    "    set.seed(seed)\n",
    "    text <- text[sample(length(text), sample.size)]\n",
    "  }\n",
    "  ## the terms before the first '\\t' are the labels (the newsgroup names) and all the remaining text after '\\t' are the actual documents\n",
    "  docs <- strsplit(text, '\\t')\n",
    "  # store the labels for evaluation\n",
    "  labels <-  lapply(docs, function(x) x[1])\n",
    "                           \n",
    "  # store the unlabeled texts \n",
    "                           \n",
    "  doc.id <- seq(1,length(docs))\n",
    "                           \n",
    "  docs <- data.frame(doc_id = doc.id, text = unlist(lapply(docs, function(x) x[2])))\n",
    "                          \n",
    "  # create a corpus\n",
    "  docs <- DataframeSource(docs)\n",
    "  corp <- Corpus(docs)\n",
    "  \n",
    "  # Preprocessing:\n",
    "  if (pre.proc){\n",
    "    corp <- tm_map(corp, removeWords, stopwords(\"english\")) # remove stop words (the most common word in a language that can be find in any document)\n",
    "    corp <- tm_map(corp, removePunctuation) # remove pnctuation\n",
    "    corp <- tm_map(corp, stemDocument) # perform stemming (reducing inflected and derived words to their root form)\n",
    "    corp <- tm_map(corp, removeNumbers) # remove all numbers\n",
    "    corp <- tm_map(corp, stripWhitespace) # remove redundant spaces \n",
    "  }  \n",
    "  # Create a matrix which its rows are the documents and colomns are the words. \n",
    "  dtm <- DocumentTermMatrix(corp)\n",
    "  ## reduce the sparcity of out dtm\n",
    "  dtm <- removeSparseTerms(dtm, spr.ratio)\n",
    "  ## convert dtm to a matrix\n",
    "  word.doc.mat <- t(as.matrix(dtm))\n",
    "  \n",
    "  # Return the result\n",
    "  return (list(\"docs\" = docs, \"word.doc.mat\"= word.doc.mat, \"labels\" = labels))\n",
    "}\n",
    "\n",
    "## --- helper function ------------------------------------------------------------------ \n",
    "# Input:    logA1, logA2 ... logAn\n",
    "# Output:   log(A1+A2+...+An)\n",
    "#\n",
    "# This function is needed to prevent numerical overflow/underflow when working with small numbers, \n",
    "# because we can easily get small numbers by multiplying p1 * p2 * ... * pn (where 0 <= pi <= 1 are probabilities).   \n",
    "#\n",
    "# Example: Suppose we are interested in p1*p2*p3 + q1*q2+q3 where all numbers are probabilities \\in [0,1]\n",
    "#          To prevent numerical errors, we do the computation in the log space and convert the result back using the exp function \n",
    "#          Hence our approach is to form the vector v = [log(p1)+log(p2)+log(p3) , log(q1)+log(q2)+log(q3)] \n",
    "#          Then get the results by: exp(logSum(v))\n",
    "logSum <- function(v) {\n",
    "   m = max(v)\n",
    "   return ( m + log(sum(exp(v-m))))\n",
    "}\n",
    "\n",
    "##--- Initialize model parameters randomly --------------------------------------------\n",
    "initial.param <- function(vocab_size, K=4, seed=123456){\n",
    "  rho <- matrix(1/K,nrow = K, ncol=1)                    # assume all clusters have the same size (we will update this later on)\n",
    "  mu <- matrix(runif(K*vocab_size),nrow = K, ncol = vocab_size)    # initiate Mu \n",
    "  mu <- prop.table(mu, margin = 1)               # normalization to ensure that sum of each row is 1\n",
    "  return (list(\"rho\" = rho, \"mu\"= mu))\n",
    "}\n",
    "\n",
    "##--- E Step for Document Clustering  --------------------------------------------\n",
    "# this function currently implements the E-step of the soft-EM\n",
    "# Student needs to modify this function if wants to make it Hard-EM \n",
    "#\n",
    "E.step <- function(gamma, model, counts, type = 'soft'){\n",
    "  # Model Parameter Setting\n",
    "  N <- dim(counts)[2] # number of documents\n",
    "  K <- dim(model$mu)[1]\n",
    "\n",
    "  # E step:    \n",
    "  for (n in 1:N){\n",
    "    for (k in 1:K){\n",
    "      ## calculate the posterior based on the estimated mu and rho in the \"log space\"\n",
    "      gamma[n,k] <- log(model$rho[k,1]) +  sum(counts[,n] * log(model$mu[k,])) \n",
    "    }\n",
    "    # normalisation to sum to 1 in the log space\n",
    "    logZ = logSum(gamma[n,])\n",
    "    gamma[n,] = gamma[n,] - logZ\n",
    "  }\n",
    "  \n",
    "  # converting back from the log space \n",
    "  gamma <- exp(gamma)\n",
    "        \n",
    " if(type == 'hard'){\n",
    "    max.prob <- gamma == apply(gamma, 1, max) # for each point find the cluster with the maximum (estimated) probability\n",
    "    gamma[max.prob] <- 1 # assign each point to the cluster with the highest probability\n",
    "    gamma[!max.prob] <- 0 # remove points from clusters with lower probabilites\n",
    " }\n",
    "    \n",
    "    \n",
    "  return (gamma)\n",
    "}\n",
    "\n",
    "##--- M Step for Document Clustering  --------------------------------------------\n",
    "M.step <- function(gamma, model, counts){\n",
    "  # Model Parameter Setting\n",
    "  N <- dim(counts)[2]   # number of documents\n",
    "  W <- dim(counts)[1]   # number of words i.e. vocabulary size\n",
    "  K <- dim(model$mu)[1] # number of clusters\n",
    "\n",
    " \n",
    "  # M step: Student needs to write this part for soft/hard EM\n",
    "  #......\n",
    "  #\n",
    "  # hint: before you normalise a vector so that it sums to 1, first add a small number (eps) to all elements of the vector.\n",
    "  # for example, suppose you have a vector [n1,n2,n3] and you want to normalise it to make it a probability distribution. \n",
    "  # you first need to add eps to elements [n1+eps,n2+eps,n3+eps], then divide the elements by (n1+n2+n3+ 3*eps) so that the vecotr sums to 1. \n",
    "  # this prevents NaN for vectors where all elements aer zero such as [0,0,0] because after adding eps you have [eps,eps,eps] which \n",
    "  # results in the uniform distribution after normalisation.\n",
    "    \n",
    "  for (k in 1:K){\n",
    "        # the relative cluster size  \n",
    "        model$rho[k,1] <- sum(gamma[,k])/N       \n",
    "    }\n",
    "    \n",
    "    # for mu\n",
    "    for (k in 1:K){\n",
    "        # initialze numerator of mu for each cluster (K) to 0\n",
    "        num = 0\n",
    "        \n",
    "        # For every document\n",
    "        for (n in 1:N){\n",
    "            \n",
    "            # numerator of mu = sum of (posterior prob (gamma) of each document in the cluster) X (each words in the document)\n",
    "            num = num + gamma[n,k] * counts[,n]\n",
    "        }\n",
    "        \n",
    "        # add a small number (eps = 1e-10)\n",
    "        num = num + 1e-10\n",
    "        \n",
    "        # Probablity of each work occuring in a document assigned to cluster k \n",
    "        model$mu[k,] <- num/sum(num)\n",
    "        \n",
    "    }  \n",
    "\n",
    "\n",
    "  # Return the result\n",
    "  return (model)\n",
    "}\n",
    "                                                           \n",
    "##--- EM for Document Clustering --------------------------------------------\n",
    "EM <- function(counts, K=4, max.epoch=10, type = 'soft', seed=123456){\n",
    "  #INPUTS:\n",
    "  ## counts: word count matrix\n",
    "  ## K: the number of clusters\n",
    "  #OUTPUTS:\n",
    "  ## model: a list of model parameters\n",
    "  \n",
    "  # Model Parameter Setting\n",
    "  N <- dim(counts)[2] # number of documents\n",
    "  W <- dim(counts)[1] # number of unique words (in all documents)\n",
    "  \n",
    "  # Initialization\n",
    "  model <- initial.param(W, K=K, seed=seed)\n",
    "  gamma <- matrix(0, nrow = N, ncol = K)\n",
    "\n",
    "  print(train_obj(model,counts))\n",
    "  # Build the model\n",
    "  for(epoch in 1:max.epoch){\n",
    "    \n",
    "    # E Step for soft classification\n",
    "    gamma <- E.step(gamma, model, counts, type)\n",
    "\n",
    "    # M Step\n",
    "    model <- M.step(gamma, model, counts)\n",
    "   \n",
    "    print(train_obj(model,counts)) \n",
    "  }\n",
    "  # Return Model\n",
    "  return(list(\"model\"=model,\"gamma\"=gamma))\n",
    "}\n",
    "\n",
    "##--- the training objective function --------------------------------------------\n",
    "# Input: \n",
    "#    model:  the model object containing the mu and rho\n",
    "#    counts: the word-document frequency matrix\n",
    "# Output:\n",
    "#    nloglike: the negative log-likelihood i.e. log P(counts|model) \n",
    "#   \n",
    "train_obj <- function(model, counts) { \n",
    "  N <- dim(counts)[2] # number of documents\n",
    "  K <- dim(model$mu)[1]\n",
    "   \n",
    "  nloglike = 0\n",
    "  for (n in 1:N){\n",
    "    lprob <- matrix(0,ncol = 1, nrow=K)\n",
    "    for (k in 1:K){\n",
    "      lprob[k,1] = sum(counts[,n] * log(model$mu[k,])) \n",
    "    }\n",
    "    nloglike <- nloglike - logSum(lprob + log(model$rho))\n",
    "  }\n",
    "  \n",
    "  return (nloglike)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 EM Hard Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(con, \"r\"):\n",
      "\"cannot open file './Task2A.txt': No such file or directory\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(con, \"r\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(con, \"r\"): cannot open the connection\nTraceback:\n",
      "1. read.data(file.name = \"./Task2A.txt\", sample.size = 0, seed = 100, \n .     pre.proc = TRUE, spr.ratio = 0.99)",
      "2. readLines(file.name)   # at line 31 of file <text>",
      "3. file(con, \"r\")"
     ]
    }
   ],
   "source": [
    "### main body ##################################################################\n",
    "\n",
    "# Reading documents \n",
    "## Note: sample.size=0 means all read all documents!\n",
    "##(for develiopment and debugging use a smaller subset e.g., sample.size = 40)\n",
    "data <- read.data(file.name='./Task2A.txt', sample.size=0, seed=100, pre.proc=TRUE, spr.ratio= .99)\n",
    "\n",
    "# word-document frequency matrix \n",
    "counts <- data$word.doc.mat        \n",
    "# below is toy data if you want to work with\n",
    "# counts <- matrix(c(1,1,0,1,0,0,0,1,1,1,0,1,0,0,1,1,0,0),nrow=3,ncol=6)\n",
    "\n",
    "# calling the EM algorithm on the data\n",
    "res.h <- EM(counts, K=4, max.epoch=5, type = 'hard')   \n",
    "\n",
    "# visualization\n",
    "## find the culster with the maximum probability (since we have soft assignment here)\n",
    "label.hat.hard <- apply(res.h$gamma, 1, which.max) \n",
    "## normalize the count matrix for better visualization\n",
    "counts<-scale(counts) # only use when the dimensionality of the data (number of words) is large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EM Soft Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(con, \"r\"):\n",
      "\"cannot open file './Task2A.txt': No such file or directory\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(con, \"r\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(con, \"r\"): cannot open the connection\nTraceback:\n",
      "1. read.data(file.name = \"./Task2A.txt\", sample.size = 0, seed = 100, \n .     pre.proc = TRUE, spr.ratio = 0.99)",
      "2. readLines(file.name)   # at line 31 of file <text>",
      "3. file(con, \"r\")"
     ]
    }
   ],
   "source": [
    "### main body ##################################################################\n",
    "\n",
    "# Reading documents \n",
    "## Note: sample.size=0 means all read all documents!\n",
    "##(for develiopment and debugging use a smaller subset e.g., sample.size = 40)\n",
    "data <- read.data(file.name='./Task2A.txt', sample.size=0, seed=100, pre.proc=TRUE, spr.ratio= .99)\n",
    "\n",
    "# word-document frequency matrix \n",
    "counts <- data$word.doc.mat        \n",
    "# below is toy data if you want to work with\n",
    "# counts <- matrix(c(1,1,0,1,0,0,0,1,1,1,0,1,0,0,1,1,0,0),nrow=3,ncol=6)\n",
    "\n",
    "# calling the EM algorithm on the data\n",
    "res <- EM(counts, K=4, max.epoch=5,type='soft')   \n",
    "\n",
    "# visualization\n",
    "## find the culster with the maximum probability (since we have soft assignment here)\n",
    "label.hat.soft <- apply(res$gamma, 1, which.max) \n",
    "## normalize the count matrix for better visualization\n",
    "counts<-scale(counts) # only use when the dimensionality of the data (number of words) is large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Visualisation using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--- Cluster Visualization -------------------------------------------------\n",
    "cluster.viz <- function(doc.word.mat, color.vector, title=' '){\n",
    "  p.comp <- prcomp(doc.word.mat, scale. = TRUE, center = TRUE)\n",
    "  plot(p.comp$x, col=color.vector, pch=1,  main=title)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in t(counts): object 'counts' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in t(counts): object 'counts' not found\nTraceback:\n",
      "1. cluster.viz(t(counts), label.hat.hard, \"Estimated Clusters Hard EM\")",
      "2. prcomp(doc.word.mat, scale. = TRUE, center = TRUE)   # at line 3 of file <text>",
      "3. t(counts)"
     ]
    }
   ],
   "source": [
    "## visualize the stimated clusters\n",
    "cluster.viz(t(counts), label.hat.hard, 'Estimated Clusters Hard EM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in t(counts): object 'counts' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in t(counts): object 'counts' not found\nTraceback:\n",
      "1. cluster.viz(t(counts), label.hat.soft, \"Estimated Clusters Soft EM\")",
      "2. prcomp(doc.word.mat, scale. = TRUE, center = TRUE)   # at line 3 of file <text>",
      "3. t(counts)   # at line 3 of file <text>"
     ]
    }
   ],
   "source": [
    "## visualize the stimated clusters\n",
    "cluster.viz(t(counts), label.hat.soft, 'Estimated Clusters Soft EM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in t(counts): object 'counts' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in t(counts): object 'counts' not found\nTraceback:\n",
      "1. cluster.viz(t(counts), factor(unlist(data$labels)), \"Real Clusters\")",
      "2. prcomp(doc.word.mat, scale. = TRUE, center = TRUE)   # at line 3 of file <text>",
      "3. t(counts)   # at line 3 of file <text>"
     ]
    }
   ],
   "source": [
    "## visualize the real clusters\n",
    "cluster.viz(t(counts), factor(unlist(data$labels)), 'Real Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
